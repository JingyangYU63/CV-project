{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f06428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.datasets as dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90839c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 17801\n",
       "    Root location: /Users/yujingyang/Downloads/IMAGES\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = '/Users/yujingyang/Downloads/IMAGES'\n",
    "image_size = (64, 64)\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=T.Compose([\n",
    "                               T.Resize(image_size),\n",
    "                               T.ToTensor(),\n",
    "                               T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6a28fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.zeros((len(dataset), 3, 64, 64))\n",
    "labels = torch.zeros(len(dataset)).reshape(-1)\n",
    "for i in range(len(dataset)):\n",
    "    images[i] = dataset[i][0]\n",
    "    labels[i] = dataset[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37c6e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(images, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac39041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(labels, 'labels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
